# References:
  # https://github.com/moein-shariatnia/OpenAI-CLIP

SEED: 333
ARCHITECTURE:
  DROP_PROB: 0.1
  IMG_ENC:
    IMG_SIZE: 224 # ViT-B/32
    PATCH_SIZE: 32 # ViT-B/32
    N_LAYERS: 12 # ViT-B/32
    N_HEADS: 12 # ViT-B/32
    HIDDEN_DIM: 768 # ViT-B/32
    MLP_DIM: 3072 # ViT-B/32
  TEXT_ENC:
    # "The transformer operates on a lower-cased byte pair encoding (BPE) representation of the text with a
    # 49,152 vocab size"
    # 저는 WordPiece 기반의 DisbilBERT tokenizer를 사용하고 vocabulary size 또한 가져가겠습니다.
    VOCAB_SIZE: 30522
    # "For computational efficiency, the max sequence length was capped at 76."
    # Max length of texts in Flickr8k dataset is 44. I'll use 64
    MAX_LEN: 64
    N_LAYERS: 6 # DistilBERT
    N_HEADS: 12 # DistilBERT
    HIDDEN_DIM: 768 # DistilBERT
    MLP_DIM: 3072 # DistilBERT
  EMBED_DIM: 256
TRAINING:
  # LR: 0.00005 # "$5 \times 10^{-4}$"
  # LR: 0.0001 # "$5 \times 10^{-4}$"
  LR: 0.0005 # "$5 \times 10^{-4}$"
  # IMG_ENC_LR: 0.0001
  # TEXT_ENC_LR: 0.00001
  MAX_TEMP: 100 # "Maximum temperature"
  WARMUP_STEPS: 2000 # "Warm-up iterations"
OPTIMIZER:
  BETA1: 0.9
  BETA2: 0.98
  EPS: 0.000001 # "Adam $\epsilon"
  WEIGHT_DECAY: 0.2 # "Weight decay"
